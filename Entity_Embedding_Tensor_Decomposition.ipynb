{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKpUJCfSfoKY"
      },
      "source": [
        "# 1. Load Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2MIZsjusE1k",
        "outputId": "a03c7fee-b5cf-47c2-be8d-064a9f81044d"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3784\\3544663770.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "from sys import getsizeof\n",
        "import ast\n",
        "import os\n",
        "import joblib\n",
        "\n",
        "!pip install -q -U torch watermark\n",
        "%reload_ext watermark\n",
        "\n",
        "!pip install scikit-tensor-py3\n",
        "from sktensor import dtensor, cp_als\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sufdgG8mRTy",
        "outputId": "800c68e2-00d9-4710-ccb7-6faa3051875e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%watermark` not found.\n"
          ]
        }
      ],
      "source": [
        "%watermark -v -p pandas,numpy,sklearn,matplotlib,seaborn,sktensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrxDs0zCf0DT"
      },
      "source": [
        "# 2. Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "emP-xfYDsLkC"
      },
      "outputs": [],
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "def show_memory(unit='MB', threshold='MB'):\n",
        "    '''check memory usage\n",
        "\n",
        "    :param unit: memory unit, `B`,`KB`,`MB`,`GB`\n",
        "    :param threshold: only show memory usage greater than the threshold\n",
        "    '''\n",
        "\n",
        "    scale = {'B': 1, 'KB': 1024, 'MB': 1048576, 'GB': 1073741824}\n",
        "    for i in list(globals().keys()):\n",
        "        memory = eval('getsizeof({})'.format(i))\n",
        "        if memory >= scale[threshold]:\n",
        "            print(i, str(memory//scale[unit]) + ' ' + unit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjVLdsmif5JB"
      },
      "source": [
        "# 3. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "1v2NXWOCw31k",
        "outputId": "8e074125-3dca-4624-c67d-75a2b465b553"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m file_path_train \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/content/drive/My Drive/Colab Notebooks/DATA5703/Data/Dataset/TrainData_1021.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      4\u001b[0m file_path_test \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/content/drive/My Drive/Colab Notebooks/DATA5703/Data/Dataset/TestData_1021.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m train_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(file_path_train)\n\u001b[0;32m      7\u001b[0m test_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(file_path_test)\n\u001b[0;32m      9\u001b[0m data_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([train_df, test_df], ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "# Read data\n",
        "\n",
        "file_path_train = '/content/drive/My Drive/Colab Notebooks/DATA5703/Data/Dataset/TrainData_1021.csv'\n",
        "file_path_test = '/content/drive/My Drive/Colab Notebooks/DATA5703/Data/Dataset/TestData_1021.csv'\n",
        "\n",
        "train_df = pd.read_csv(file_path_train)\n",
        "test_df = pd.read_csv(file_path_test)\n",
        "\n",
        "data_df = pd.concat([train_df, test_df], ignore_index=True)\n",
        "\n",
        "data_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYoenfW9zSKn",
        "outputId": "e96019d4-f2c4-41d3-99df-1a7273f2f547"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "print(data_df.shape)\n",
        "show_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "1BWcpE3DytMN",
        "outputId": "7c570360-1a92-496b-bd39-3267c36b25aa"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'time' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstr_to_arr\u001b[39m(str_ls):\n\u001b[0;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mfromstring(str_ls[\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     10\u001b[0m data_df[\u001b[39m'\u001b[39m\u001b[39mheart_rate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data_df\u001b[39m.\u001b[39mapply(\n\u001b[0;32m     11\u001b[0m     \u001b[39mlambda\u001b[39;00m x: str_to_arr(x[\u001b[39m'\u001b[39m\u001b[39mheart_rate\u001b[39m\u001b[39m'\u001b[39m]), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m data_df[\u001b[39m'\u001b[39m\u001b[39maltitude\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data_df\u001b[39m.\u001b[39mapply(\n\u001b[0;32m     13\u001b[0m     \u001b[39mlambda\u001b[39;00m x: str_to_arr(x[\u001b[39m'\u001b[39m\u001b[39maltitude\u001b[39m\u001b[39m'\u001b[39m]), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
          ]
        }
      ],
      "source": [
        "# sequence data was stored in string formatted list\n",
        "# convert sequence data to array\n",
        "\n",
        "def str_to_arr(str_ls):\n",
        "    return np.fromstring(str_ls[1:-1], dtype=np.float, sep=',')\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "data_df['heart_rate'] = data_df.apply(lambda x: str_to_arr(x['heart_rate']), axis=1)\n",
        "data_df['altitude'] = data_df.apply(lambda x: str_to_arr(x['altitude']), axis=1)\n",
        "data_df['derived_speed'] = data_df.apply(lambda x: str_to_arr(x['derived_speed']), axis=1)\n",
        "data_df['derived_distance'] = data_df.apply(lambda x: str_to_arr(x['derived_distance']), axis=1)\n",
        "data_df['timestamp'] = data_df.apply(lambda x: str_to_arr(x['timestamp']), axis=1)\n",
        "\n",
        "elapsed = format_time(time.time() - start)\n",
        "print(elapsed)\n",
        "\n",
        "data_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soJV2ms-KSW5"
      },
      "source": [
        "# 4. Min-Max scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg4tMYsdKGt2",
        "outputId": "ab581701-4bb2-494e-bc69-78cd3a0792f5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mdir\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/content/drive/My Drive/Colab Notebooks/DATA5703/scaler_model/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m features:\n\u001b[1;32m---> 10\u001b[0m     path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mdir\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mscaler_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mfeature\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_2.m\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m     scaler_dic[feature] \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39mload(path)\n\u001b[0;32m     13\u001b[0m scaler_dic\n",
            "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ],
      "source": [
        "# Load Min-max scaler models\n",
        "\n",
        "scaler_dic = dict()\n",
        "features = ['calories', 'distance', 'duration', 'heart_rate',\n",
        "            'time_elapsed', 'altitude', 'derived_distance', 'speed']\n",
        "\n",
        "dir = '/content/drive/My Drive/Colab Notebooks/DATA5703/scaler_model/'\n",
        "\n",
        "for feature in features:\n",
        "    path = os.path.join(dir, 'scaler_'+feature+'_2.m')\n",
        "    scaler_dic[feature] = joblib.load(path)\n",
        "\n",
        "scaler_dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "eSzTrDq0KGze",
        "outputId": "ec78e2a5-f5a2-4054-ca24-fcbe8a349eb8"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "sequence_df = data_df[['altitude', 'heart_rate',\n",
        "                       'derived_distance', 'derived_speed']].copy()\n",
        "sequence_df.rename(columns={'derived_speed': 'speed'}, inplace=True)\n",
        "\n",
        "context_df = data_df[['userId', 'gender', 'sport',\n",
        "                      'duration', 'calories', 'distance', 'Route_id']].copy()\n",
        "\n",
        "display(sequence_df.head(1))\n",
        "display(context_df.head(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "RLMYPyoEKG2T",
        "outputId": "4150d56c-9291-44ab-ef9e-a04c784c782b"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Apply min max scaler for each feature\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# for features in sequence_df\n",
        "for feature in sequence_df.columns:\n",
        "    # Reshape data using array.reshape(-1, 1) if data has a single feature\n",
        "    sequence_df[feature] = sequence_df.apply(lambda x: np.concatenate(scaler_dic[feature].transform(x[feature].reshape(-1, 1)), axis=0)[0:499], axis=1)\n",
        "\n",
        "# for features in context_df\n",
        "for feature in context_df[context_df.columns.difference(['userId', 'gender', 'sport', 'Route_id'])].columns:\n",
        "    # Reshape data using array.reshape(-1, 1) if data has a single feature\n",
        "    context_df[feature] = scaler_dic[feature].transform(context_df[feature].to_numpy().reshape(-1, 1))\n",
        "\n",
        "\n",
        "elapsed = format_time(time.time() - start)\n",
        "print(elapsed)\n",
        "\n",
        "display(sequence_df.head(1))\n",
        "display(context_df.head(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "iTHrCYWGKG5L",
        "outputId": "4e0822bf-1011-4a62-f8b2-8df6817d6bf1"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "data_df = pd.concat([context_df, sequence_df, data_df[['timestamp']]], axis=1)\n",
        "\n",
        "data_df.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggN2gyGtgJJM"
      },
      "source": [
        "# 5. Process data to construct User-Workout Route-Context Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "PmrsSQLO4NwM",
        "outputId": "fadcf97e-0c7e-45dc-8fb3-6317db595f53"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'time' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[24], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdeltas: \u001b[39m\u001b[39m'\u001b[39m, deltas)\n\u001b[0;32m     27\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     31\u001b[0m userlist \u001b[39m=\u001b[39m data_df\u001b[39m.\u001b[39muserId\u001b[39m.\u001b[39munique()\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m     32\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mThere are \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m users.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(userlist)))\n",
            "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
          ]
        }
      ],
      "source": [
        "# Construct user profile, which will be used in embedding visualisation\n",
        "\n",
        "# Count workout numbers and workout frequency for each user\n",
        "\n",
        "def get_freq_from_ts(ts_list):\n",
        "\n",
        "    deltas = []\n",
        "\n",
        "    ts_list.sort()\n",
        "    for i, ts in enumerate(ts_list):\n",
        "        if i == 0:\n",
        "            pass\n",
        "        else:\n",
        "            ts_prev = ts_list[i-1]\n",
        "\n",
        "            dt_prev = datetime.datetime.fromtimestamp(ts_prev).date()\n",
        "            dt = datetime.datetime.fromtimestamp(ts).date()\n",
        "\n",
        "            deltas.append(abs(dt-dt_prev).days)\n",
        "    try:\n",
        "        if (len(deltas) == 1) & (sum(deltas) == 0):\n",
        "            return 0\n",
        "        else:\n",
        "            return 1./(sum(deltas)/len(deltas))\n",
        "    except:\n",
        "        print('deltas: ', deltas)\n",
        "        raise\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "userlist = data_df.userId.unique().tolist()\n",
        "print('There are {} users.'.format(len(userlist)))\n",
        "\n",
        "user_profile = []\n",
        "\n",
        "for user in userlist:\n",
        "    time_stamps_list = data_df[data_df.userId == user].timestamp.tolist()\n",
        "    time_stamp_list = [time_stamps[0] for time_stamps in time_stamps_list]\n",
        "    gender = data_df[data_df.userId == user].gender.iloc[0]\n",
        "    records = len(time_stamp_list)\n",
        "    if records > 1:\n",
        "        user_profile.append([user,records, get_freq_from_ts(time_stamp_list)])\n",
        "    else:\n",
        "        user_profile.append([user, 1,0])\n",
        "\n",
        "user_profile_df = pd.DataFrame(user_profile, columns = ['userId','workout_records', 'workout_freq'])\n",
        "\n",
        "elapsed = format_time(time.time() - start)\n",
        "print(elapsed)\n",
        "\n",
        "user_profile_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "uUB3T_QfjcIC",
        "outputId": "3f7fdee6-cd45-4e4f-99f8-b1efa23cbf82"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Compute avg_calories, avg_speed, avg_distance for each user\n",
        "\n",
        "user_profile_list = []\n",
        "\n",
        "for user in userlist:\n",
        "\n",
        "    avg_calories = data_df[data_df.userId == user].calories.mean()\n",
        "    avg_distance = data_df[data_df.userId == user].distance.mean()\n",
        "    avg_speed = data_df[data_df.userId == user].apply(lambda x: x.speed.mean(), axis=1).mean()\n",
        "\n",
        "    user_arr = np.array([user, avg_calories,avg_speed,avg_distance])\n",
        "\n",
        "    user_profile_list.append(user_arr)\n",
        "\n",
        "user_df = pd.DataFrame(user_profile_list, columns=['userId', 'avg_calories','avg_speed','avg_distance'])\n",
        "user_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "thQn4HUFpQ10",
        "outputId": "c0a80e69-5c78-4de0-d587-5bd8b02ac595"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Add additional features to user_profile_df\n",
        "\n",
        "user_profile_df = user_profile_df.join(user_df.set_index('userId'), how='left', on='userId')\n",
        "\n",
        "user_profile_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "zfzJrfym4Nzm",
        "outputId": "76a0fcf3-89a5-46e8-a4c1-f667ab6f44ce"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Add workout numbers and workout frequency to data_df\n",
        "start = time.time()\n",
        "\n",
        "data_df['workout_records'] = data_df.apply(lambda x: user_profile_df[user_profile_df.userId == x.userId].workout_records.values[0], axis=1)\n",
        "data_df['workout_freq'] = data_df.apply(lambda x: user_profile_df[user_profile_df.userId == x.userId].workout_freq.values[0], axis=1)\n",
        "\n",
        "elapsed = format_time(time.time() - start)\n",
        "print(elapsed)\n",
        "\n",
        "data_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoAYKhl_4N2i"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Scale workout_records and workout_freq\n",
        "\n",
        "scaler_dic = {}\n",
        "\n",
        "scaler_workout_records = MinMaxScaler()\n",
        "scaler_workout_freq = MinMaxScaler()\n",
        "\n",
        "scaler_workout_records.fit(data_df.workout_records.to_numpy().reshape(-1, 1))\n",
        "scaler_workout_freq.fit(data_df.workout_freq.to_numpy().reshape(-1, 1))\n",
        "\n",
        "data_df['workout_records'] = scaler_workout_records.transform(data_df.workout_records.to_numpy().reshape(-1, 1)).reshape(1, -1)[0]\n",
        "data_df['workout_freq'] = scaler_workout_freq.transform(data_df.workout_freq.to_numpy().reshape(-1, 1)).reshape(1, -1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II-yGmFG73y_",
        "outputId": "1bf5f416-a7a9-404d-fbd8-95b6d60f5351"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Compute some parameters from sequence data\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "data_df['min_altitude'] = data_df.apply(lambda x: np.amin(x.altitude), axis=1)\n",
        "data_df['avg_altitude'] = data_df.apply(lambda x: x.altitude.mean(), axis=1)\n",
        "data_df['max_altitude'] = data_df.apply(lambda x: np.amax(x.altitude), axis=1)\n",
        "\n",
        "data_df['min_heart_rate'] = data_df.apply(lambda x: np.amin(x.heart_rate), axis=1)\n",
        "data_df['avg_heart_rate'] = data_df.apply(lambda x: x.heart_rate.mean(), axis=1)\n",
        "data_df['max_heart_rate'] = data_df.apply(lambda x: np.amax(x.heart_rate), axis=1)\n",
        "\n",
        "data_df['min_distance'] = data_df.apply(lambda x: np.amin(x.derived_distance), axis=1)\n",
        "data_df['avg_distance'] = data_df.apply(lambda x: x.derived_distance.mean(), axis=1)\n",
        "data_df['max_distance'] = data_df.apply(lambda x: np.amax(x.derived_distance), axis=1)\n",
        "\n",
        "data_df['min_speed'] = data_df.apply(lambda x: np.amin(x.speed), axis=1)\n",
        "data_df['avg_speed'] = data_df.apply(lambda x: x.speed.mean(), axis=1)\n",
        "data_df['max_speed'] = data_df.apply(lambda x: np.amax(x.speed), axis=1)\n",
        "\n",
        "elapsed = format_time(time.time() - start)\n",
        "print(elapsed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "BZogm91-FFuu",
        "outputId": "6c11e1aa-150f-4c4d-ac8f-ea81c5f743c9"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Construct route profile, which will be used in embedding visualisation\n",
        "\n",
        "route_list = data_df.Route_id.unique()\n",
        "route_profile_list = []\n",
        "\n",
        "for route in route_list:\n",
        "    tmp_df = data_df[data_df.Route_id == route].copy()\n",
        "    avg_distance_route = tmp_df.avg_distance.mean()\n",
        "    avg_speed_route = tmp_df.avg_speed.mean()\n",
        "    avg_calories_route = tmp_df.calories.mean()\n",
        "    sport = tmp_df.sport.iloc[0]\n",
        "\n",
        "    route_arr = np.array([route,sport,avg_calories_route,avg_speed_route,avg_distance_route])\n",
        "\n",
        "    route_profile_list.append(route_arr)\n",
        "\n",
        "route_profile_df = pd.DataFrame(route_profile_list, columns=['Route_id','sport','avg_calories','avg_speed','avg_distance'])\n",
        "\n",
        "route_profile_df['Route_id'] = route_profile_df['Route_id'].astype(int)\n",
        "route_profile_df[['avg_calories', 'avg_speed', 'avg_distance']] = route_profile_df[['avg_calories', 'avg_speed', 'avg_distance']].apply(pd.to_numeric)\n",
        "route_profile_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yA_33lXSZlA"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Encode gender feature\n",
        "\n",
        "conditions = [(data_df['gender'] == 'male'),(data_df['gender'] == 'female'),(data_df['gender'] == 'unknown')]\n",
        "\n",
        "choices = [0, 1, 2]\n",
        "\n",
        "data_df['genderId'] = np.select(conditions, choices, default=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AZ1lhjqKpc6"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Encode sport feature\n",
        "\n",
        "conditions = [(data_df['sport'] == 'run'),(data_df['sport'] == 'bike'),(data_df['sport'] == 'mountain bike')]\n",
        "\n",
        "choices = [0, 1, 2]\n",
        "\n",
        "data_df['sportId'] = np.select(conditions, choices, default=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "25joakgEErS2",
        "outputId": "d1b3fcc7-4b48-4f4f-81c0-f04e692c9929"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "data_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx4xAjG59C0S",
        "outputId": "8721f92f-157b-46af-8bcb-5b3954e92153"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Compute user - workout route - context tensor\n",
        "\n",
        "user_list = data_df.userId.unique()\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "users_routes_list = []\n",
        "route_idx = {}\n",
        "\n",
        "for route in route_list:\n",
        "    route_idx[route] = data_df[data_df.Route_id == route].sport.tolist()[0]\n",
        "\n",
        "# # mask array for tensor decomposition\n",
        "#     # array of booleans with the same shape as tensor\n",
        "#     # 0 where the values are missing and 1 everywhere else\n",
        "# mask_arr = np.ones((len(user_list), len(route_list), 13))\n",
        "\n",
        "for i, user in enumerate(user_list):\n",
        "    user_records = data_df[data_df.userId == user].shape[0]\n",
        "    user_routes_list = []\n",
        "\n",
        "    for j, route in enumerate(route_list):\n",
        "\n",
        "        user_route_df = data_df[(data_df.userId == user) & (\n",
        "            data_df.Route_id == route)].copy()\n",
        "        # if this user has more than one workout in this route\n",
        "        if user_route_df.shape[0] > 0:\n",
        "\n",
        "            user_gender = user_route_df.genderId.tolist()[0]\n",
        "            sport = user_route_df.sportId.tolist()[0]\n",
        "            workout_records = user_route_df.workout_records.tolist()[0]\n",
        "            workout_freq = user_route_df.workout_freq.tolist()[0]\n",
        "\n",
        "            cw_min_duration = user_route_df.duration.min()\n",
        "            cw_avg_duration = user_route_df.duration.mean()\n",
        "            cw_max_duration = user_route_df.duration.max()\n",
        "\n",
        "            cw_min_calories = user_route_df.calories.min()\n",
        "            cw_avg_calories = user_route_df.calories.mean()\n",
        "            cw_max_calories = user_route_df.calories.max()\n",
        "\n",
        "            cw_min_distance = user_route_df.distance.min()\n",
        "            cw_avg_distance = user_route_df.distance.mean()\n",
        "            cw_max_distance = user_route_df.distance.max()\n",
        "\n",
        "            cw_min_min_altitude = user_route_df.min_altitude.min()\n",
        "            cw_avg_min_altitude = user_route_df.min_altitude.mean()\n",
        "            cw_max_min_altitude = user_route_df.min_altitude.max()\n",
        "\n",
        "            cw_min_avg_altitude = user_route_df.avg_altitude.min()\n",
        "            cw_avg_avg_altitude = user_route_df.avg_altitude.mean()\n",
        "            cw_max_avg_altitude = user_route_df.avg_altitude.max()\n",
        "\n",
        "            cw_min_max_altitude = user_route_df.max_altitude.min()\n",
        "            cw_avg_max_altitude = user_route_df.max_altitude.mean()\n",
        "            cw_max_max_altitude = user_route_df.max_altitude.max()\n",
        "\n",
        "            cw_min_min_heart_rate = user_route_df.min_heart_rate.min()\n",
        "            cw_avg_min_heart_rate = user_route_df.min_heart_rate.mean()\n",
        "            cw_max_min_heart_rate = user_route_df.min_heart_rate.max()\n",
        "\n",
        "            cw_min_avg_heart_rate = user_route_df.avg_heart_rate.min()\n",
        "            cw_avg_avg_heart_rate = user_route_df.avg_heart_rate.mean()\n",
        "            cw_max_avg_heart_rate = user_route_df.avg_heart_rate.max()\n",
        "\n",
        "            cw_min_max_heart_rate = user_route_df.max_heart_rate.min()\n",
        "            cw_avg_max_heart_rate = user_route_df.max_heart_rate.mean()\n",
        "            cw_max_max_heart_rate = user_route_df.max_heart_rate.max()\n",
        "\n",
        "            cw_min_min_distance = user_route_df.min_distance.min()\n",
        "            cw_avg_min_distance = user_route_df.min_distance.mean()\n",
        "            cw_max_min_distance = user_route_df.min_distance.max()\n",
        "\n",
        "            cw_min_avg_distance = user_route_df.avg_distance.min()\n",
        "            cw_avg_avg_distance = user_route_df.avg_distance.mean()\n",
        "            cw_max_avg_distance = user_route_df.avg_distance.max()\n",
        "\n",
        "            cw_min_max_distance = user_route_df.max_distance.min()\n",
        "            cw_avg_max_distance = user_route_df.max_distance.mean()\n",
        "            cw_max_max_distance = user_route_df.max_distance.max()\n",
        "\n",
        "            cw_min_min_speed = user_route_df.min_speed.min()\n",
        "            cw_avg_min_speed = user_route_df.min_speed.mean()\n",
        "            cw_max_min_speed = user_route_df.min_speed.max()\n",
        "\n",
        "            cw_min_avg_speed = user_route_df.avg_speed.min()\n",
        "            cw_avg_avg_speed = user_route_df.avg_speed.mean()\n",
        "            cw_max_avg_speed = user_route_df.avg_speed.max()\n",
        "\n",
        "            cw_min_max_speed = user_route_df.max_speed.min()\n",
        "            cw_avg_max_speed = user_route_df.max_speed.mean()\n",
        "            cw_max_max_speed = user_route_df.max_speed.max()\n",
        "            \n",
        "            # proportion of this route in all workout route records of this user\n",
        "            cw_route_weight = user_route_df.shape[0]/user_records\n",
        "\n",
        "            user_route_arr = np.array([user_gender,\n",
        "                                       sport,\n",
        "                                       workout_records,\n",
        "                                       workout_freq,\n",
        "                                       cw_min_duration,\n",
        "                                       cw_avg_duration,\n",
        "                                       cw_max_duration,\n",
        "                                       cw_min_calories,\n",
        "                                       cw_avg_calories,\n",
        "                                       cw_max_calories,\n",
        "                                       cw_min_distance,\n",
        "                                       cw_avg_distance,\n",
        "                                       cw_max_distance,\n",
        "                                       cw_min_min_altitude,\n",
        "                                       cw_avg_min_altitude,\n",
        "                                       cw_max_min_altitude,\n",
        "                                       cw_min_avg_altitude,\n",
        "                                       cw_avg_avg_altitude,\n",
        "                                       cw_max_avg_altitude,\n",
        "                                       cw_min_max_altitude,\n",
        "                                       cw_avg_max_altitude,\n",
        "                                       cw_max_max_altitude,\n",
        "                                       cw_min_min_heart_rate,\n",
        "                                       cw_avg_min_heart_rate,\n",
        "                                       cw_max_min_heart_rate,\n",
        "                                       cw_min_avg_heart_rate,\n",
        "                                       cw_avg_avg_heart_rate,\n",
        "                                       cw_max_avg_heart_rate,\n",
        "                                       cw_min_max_heart_rate,\n",
        "                                       cw_avg_max_heart_rate,\n",
        "                                       cw_max_max_heart_rate,\n",
        "                                       cw_min_min_distance,\n",
        "                                       cw_avg_min_distance,\n",
        "                                       cw_max_min_distance,\n",
        "                                       cw_min_avg_distance,\n",
        "                                       cw_avg_avg_distance,\n",
        "                                       cw_max_avg_distance,\n",
        "                                       cw_min_max_distance,\n",
        "                                       cw_avg_max_distance,\n",
        "                                       cw_max_max_distance,\n",
        "                                       cw_min_min_speed,\n",
        "                                       cw_avg_min_speed,\n",
        "                                       cw_max_min_speed,\n",
        "                                       cw_min_avg_speed,\n",
        "                                       cw_avg_avg_speed,\n",
        "                                       cw_max_avg_speed,\n",
        "                                       cw_min_max_speed,\n",
        "                                       cw_avg_max_speed,\n",
        "                                       cw_max_max_speed, \n",
        "                                       cw_route_weight])\n",
        "        else:\n",
        "            user_route_arr = np.zeros([50])\n",
        "            # mask_arr[i][j] = np.zeros([50])\n",
        "\n",
        "        user_routes_list.append(user_route_arr)\n",
        "\n",
        "    user_routes_arr = np.stack(user_routes_list, axis=0)\n",
        "    users_routes_list.append(user_routes_arr)\n",
        "\n",
        "users_routes_arr = np.stack(users_routes_list, axis=0)\n",
        "\n",
        "elapsed = format_time(time.time() - start)\n",
        "print(elapsed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qxo2Ce9hgjHH"
      },
      "source": [
        "# 6. CP Decomposition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5R6f1lpx4Ls"
      },
      "source": [
        "## 6.1 Define function to perform CP Decomposition and evaluate number of CP ranks\n",
        "\n",
        "The theories of CORCONDIA and Efficient CORCONDIA algorithms in this section are referenced from\n",
        "\n",
        "1. Bro, R. and Kiers, H.A.L. (2003), A new efficient method for determining the number of components in PARAFAC models. J. Chemometrics, 17: 274-286. doi:10.1002/cem.801.\n",
        "\n",
        "2. E. E. Papalexakis and C. Faloutsos, \"Fast efficient and scalable Core Consistency Diagnostic for the parafac decomposition for big sparse tensors,\" 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brisbane, QLD, 2015, pp. 5441-5445, doi: 10.1109/ICASSP.2015.7179011.\n",
        "\n",
        "3. Anna Sapienza (2017). Tensor decomposition techniques for analysing time-varying networks. Available at : http://porto.polito.it/2668112/. doi:10.6092/polito/porto/2668112."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH3eJxUPAV19"
      },
      "source": [
        "**Core Consistency Diagnostic (CORCONDIA):**\n",
        "\n",
        "\n",
        "CP Decomposition for a three-way tensor $\\mathcal{X} \\in \\mathbb{R}^{I \\times J \\times K}$ can be represented as \n",
        "\n",
        "$$\n",
        "\\mathcal{X}=[[ \\mathbf{A}, \\mathbf{B}, \\mathbf{C} ]]+\\mathcal{E}\n",
        "$$\n",
        "\n",
        "that can be written in the matricised form\n",
        "\n",
        "$$\n",
        "\\mathrm{X}=\\mathrm{A}(\\mathrm{C} \\odot \\mathrm{B})^{T}+\\mathrm{E}\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "* $X \\in \\mathbb{R}^{I \\times JK}$: Original tensor to be decomposed in matrix form \n",
        "* $A \\in \\mathbb{R}^{I \\times R}$: Can be seen as the user matrix after CP Decomposition\n",
        "* $B \\in \\mathbb{R}^{J \\times R}$: Can be seen as the route matrix after CP Decomposition\n",
        "* $C \\in \\mathbb{R}^{K \\times R}$: Can be seen as the contextual feature matrix after CP Decomposition\n",
        "* $E \\in \\mathbb{R}^{I \\times JK}$: Decomposition Error\n",
        "\n",
        "To compute the core consistency value we first re-write the above equation as a Tucker3 model by adding a superdiagonal core tensor $\\mathcal{L} \\in \\mathbb{R}^{R \\times R \\times R}$ with superdiagonal values of **ones** in its matricised form $\\mathbf{L} \\in \\mathbb{R}^{R \\times R R}$:\n",
        "\n",
        "$$\n",
        "\\mathrm{X}=\\mathrm{A} \\mathbf{L} (\\mathrm{C} \\odot \\mathrm{B})^{T}+\\mathrm{E}\n",
        "$$\n",
        "\n",
        "Let's assume we have done the CP Decomposition with selected rank and learned matrices $\\mathrm{A}$, $\\mathrm{B}$ and $\\mathrm{C}$. Then we fit the full Tucker3 model to the data by using the CP Decomposition matrices $\\mathrm{A}$, $\\mathrm{B}$ and $\\mathrm{C}$, by minimising\n",
        "\n",
        "$$\n",
        "\\sigma(\\mathbf{G})=\\left\\|\\mathbf{X}-\\mathbf{A G}(\\mathbf{C} \\otimes \\mathbf{B})^{\\mathrm{T}}\\right\\|_{\\mathrm{F}}^{2}\n",
        "$$\n",
        "\n",
        "over core matrix $\\mathbf{G}$. This is essentially saying the residual decomposition error $\\sigma(\\mathbf{G})$ is minimised.\n",
        "\n",
        "The optimal $\\mathbf{G}$ can be computed by writing the equation in its vectorised form and applying a least squares algorithm\n",
        "\n",
        "$$\n",
        "\\sigma(\\mathbf{G})=\\|\\operatorname{vec} \\mathbf{X}-(\\mathbf{C} \\otimes \\mathbf{B} \\otimes \\mathbf{A}) \\operatorname{vec} \\mathbf{G}\\|_{F}^{2}\n",
        "$$\n",
        "\n",
        "where $\\otimes$ is **Kronecker product** (https://en.wikipedia.org/wiki/Kronecker_product).\n",
        "\n",
        "The optimal $\\mathbf{G}$ in the above equation can be determined as \n",
        "\n",
        "$$\n",
        "\\operatorname{vec} \\mathbf{G}=(\\mathbf{C} \\otimes \\mathbf{B} \\otimes \\mathbf{A})^{+} \\operatorname{vec} \\mathbf{X}\n",
        "$$\n",
        "\n",
        "when the residual decomposition error $\\sigma(\\mathbf{G})$ is minimised. Note that the superscript $+$ indicates a pseudoinverse.\n",
        "\n",
        "The underlying idea is to find the similarity between $\\mathcal{L}$ and $\\mathcal{G}$ where $\\mathcal{L}$ is a superdiagonal core tensor that all its superdiagonal values are 1 and all its off-superdiagonal values are 0. To compare the similarity, we can have a look at the distribution of the elements in the superdiagonal and off-superdiagonal of $\\mathcal{G}$. If the superdiagonal elements of $\\mathcal{G}$ are all close to the corresponding elements of $\\mathcal{L}$, which is 1, and the off-superdiagonal elements of of $\\mathcal{G}$ are all close to the corresponding elements of $\\mathcal{L}$, which is 0, then we say the CP Decomposition result is appropriate.\n",
        "\n",
        "Formally, the similarity between $\\mathcal{L}$ and $\\mathcal{G}$, or **`core consistency`** is given by\n",
        "\n",
        "$$\n",
        "c c=100\\left(1-\\frac{\\sum{_{l=1}^{R}} \\sum{_{m=1}^{R}} \\sum{_{n=1}^{R}}\\left(g_{l m n}-\\lambda_{l m n}\\right)^{2}}{R}\\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kTFoQUfAPqN"
      },
      "source": [
        "**Efficient Core Consistency (Efficient CORCONDIA):**\n",
        "\n",
        "A faster way to compute the pseudo-inverse $(\\mathbf{C} \\otimes \\mathbf{B} \\otimes \\mathbf{A})^{+}$ is \n",
        "\n",
        "$$\n",
        "\\left(\\mathbf{V}_{a} \\otimes \\mathbf{V}_{b} \\otimes \\mathbf{V}_{c}\\right)\\left(\\Sigma_{a}^{-1} \\otimes \\Sigma_{b}^{-1} \\otimes \\Sigma_{c}^{-1}\\right)\\left(\\mathbf{U}_{a}^{T} \\otimes \\mathbf{U}_{b}^{T} \\otimes \\mathbf{U}_{c}^{T}\\right)\n",
        "$$\n",
        "\n",
        "by performing $SVD$ where\n",
        "\n",
        "* $\\mathbf{A} = \\mathbf{U}_{a} \\Sigma_{a} \\mathbf{V}_{a}^T$\n",
        "* $\\mathbf{B} = \\mathbf{U}_{b} \\Sigma_{b} \\mathbf{V}_{b}^T$\n",
        "* $\\mathbf{C} = \\mathbf{U}_{c} \\Sigma_{c} \\mathbf{V}_{c}^T$\n",
        "\n",
        "Therefore, we have\n",
        "\n",
        "$$\n",
        "\\operatorname{vec} \\mathbf{G}=\\left(\\mathbf{V}_{a} \\otimes \\mathbf{V}_{b} \\otimes \\mathbf{V}_{c}\\right)\\left(\\Sigma_{a}^{-1} \\otimes \\Sigma_{b}^{-1} \\otimes \\Sigma_{c}^{-1}\\right)\\left(\\mathbf{U}_{a}^{T} \\otimes \\mathbf{U}_{b}^{T} \\otimes \\mathbf{U}_{c}^{T}\\right) \\operatorname{vec} \\mathbf{X}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGB5k5ycAE2I"
      },
      "source": [
        "This section of code was largely borrowed from https://github.com/alessandrobessi/corcondia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMB6zdx3x3hK"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "def kronecker(matrices, tensor):\n",
        "    K = len(matrices)\n",
        "    x = tensor\n",
        "    for k in range(K):\n",
        "        M = matrices[k]\n",
        "        # sktensor.core.ttm: Tensor times matrix product\n",
        "        x = x.ttm(M, k)\n",
        "    return x\n",
        "\n",
        "\n",
        "def cp_with_eff_corcondia(tensor, num_comp=1):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_comp: number of CP Decomposition components\n",
        "    tensor: tensor to be decomposed\n",
        "    '''\n",
        "\n",
        "    # perfrom CP Decomposition\n",
        "    kruskal_tensor, _, _ = cp_als(dtensor(tensor), num_comp, init='random')\n",
        "\n",
        "    A = kruskal_tensor.U[0]\n",
        "    B = kruskal_tensor.U[1]\n",
        "    C = kruskal_tensor.U[2]\n",
        "\n",
        "    # original tensor\n",
        "    x = kruskal_tensor.totensor()\n",
        "\n",
        "    # Compute Core array G with Efficient Core Consistency (efficient CORCONDIA)\n",
        "\n",
        "    # 1. Singular Value Decomposition of the paramater matrices\n",
        "    Ua, Sigma_a, Va = np.linalg.svd(A)\n",
        "    Ub, Sigma_b, Vb = np.linalg.svd(B)\n",
        "    Uc, Sigma_c, Vc = np.linalg.svd(C)\n",
        "\n",
        "    # 2. Compute Pseudo-inverse of Sigma\n",
        "    Sigma_a_Inverse = np.zeros((Ua.shape[0], Va.shape[0]), float)\n",
        "    np.fill_diagonal(Sigma_a_Inverse, Sigma_a)\n",
        "\n",
        "    Sigma_b_Inverse = np.zeros((Ub.shape[0], Vb.shape[0]), float)\n",
        "    np.fill_diagonal(Sigma_b_Inverse, Sigma_b)\n",
        "\n",
        "    Sigma_c_Inverse = np.zeros((Uc.shape[0], Vc.shape[0]), float)\n",
        "    np.fill_diagonal(Sigma_c_Inverse, Sigma_c)\n",
        "\n",
        "    Sigma_a_Inverse = np.linalg.pinv(Sigma_a_Inverse)\n",
        "    Sigma_b_Inverse = np.linalg.pinv(Sigma_b_Inverse)\n",
        "    Sigma_c_Inverse = np.linalg.pinv(Sigma_c_Inverse)\n",
        "\n",
        "    # 3. Computer Core Matrix G\n",
        "    y = kronecker([Ua.transpose(), Ub.transpose(), Uc.transpose()], x)\n",
        "    z = kronecker([Sigma_a_Inverse, Sigma_b_Inverse, Sigma_c_Inverse], y)\n",
        "    G = kronecker([Va.transpose(), Vb.transpose(), Vc.transpose()], z)\n",
        "\n",
        "    # Compute Core Consistency\n",
        "\n",
        "    # 1. Construct L (superdiagonal array of ones and off-superdiagonal of zeros)\n",
        "\n",
        "    L = np.full((num_comp, num_comp, num_comp), 0)\n",
        "    for i in range(num_comp):\n",
        "        for j in range(num_comp):\n",
        "            for l in range(num_comp):\n",
        "                if i == j == l:\n",
        "                    L[i][j][l] = 1\n",
        "\n",
        "    # 2. Compute G subtract L\n",
        "\n",
        "    c = 0\n",
        "    for i in range(num_comp):\n",
        "        for j in range(num_comp):\n",
        "            for l in range(num_comp):\n",
        "                c += float(G[i][j][l] - L[i][j][l]) ** 2.0\n",
        "\n",
        "    # 3. Compute Core Consistency\n",
        "    cc = 100 * (1 - (c / float(num_comp)))\n",
        "\n",
        "    return A, B, C, round(cc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMCYFs6ayGb7"
      },
      "source": [
        "## 6.2 Evaluate number of CP ranks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAX-aCyQ5X1_"
      },
      "source": [
        "### 6.2.1 Core Consistency Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "id": "EmTXID2ax3j2",
        "outputId": "7e16645d-e5c9-4e98-820c-4f88a74a0390"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Perform CP Decomposition and Core Consistency Computation for various ranks\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "np.random.seed(32)\n",
        "\n",
        "num_comps = np.arange(2, 21)\n",
        "core_cons = []\n",
        "\n",
        "for num_comp in num_comps:\n",
        "    _, _, _, core_consistency = cp_with_eff_corcondia(\n",
        "        users_routes_arr, num_comp)\n",
        "    core_cons.append([num_comp, core_consistency])\n",
        "\n",
        "core_cons = pd.DataFrame(core_cons, columns=['num_comp', 'core_consistency'])\n",
        "display(core_cons.head(1))\n",
        "\n",
        "elapsed = format_time(time.time() - start)\n",
        "print(elapsed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "2yKeRFw8x3my",
        "outputId": "c555f953-df64-489e-dd80-0432b441328e"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "g = sns.lineplot(data=core_cons, x='num_comp', y='core_consistency')\n",
        "\n",
        "g.axes.set_title('CP Decomposition Rank vs Core Consistency', fontsize=25)\n",
        "\n",
        "g.set_xlabel('CP Decomposition Rank', fontsize=15)\n",
        "g.set_ylabel('Core Consistency', fontsize=15)\n",
        "g.set_xticks(num_comps)\n",
        "g.set_xticklabels(num_comps)\n",
        "\n",
        "# label points on the plot\n",
        "for num_comp, cc in zip(core_cons.num_comp, core_cons.core_consistency):\n",
        "    plt.text(x=num_comp, y=cc-2, s=cc)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rRvl22T5RzS"
      },
      "source": [
        "### 6.2.2 T-SNE Plot of Embedding Matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJTjIHHwFWyX"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Create function to visualise both User Embedding and Route Embedding with T-SNE\n",
        "\n",
        "def embed_visual(profile_df, embed_df, embed_name, perplexity=5):\n",
        "\n",
        "    if embed_name == 'userEmbed':\n",
        "        id_col = 'userId'\n",
        "    elif embed_name == 'routeEmbed':\n",
        "        id_col = 'Route_id'\n",
        "\n",
        "    tmp_df = profile_df.join(embed_df.set_index(\n",
        "        id_col), how='left', on=id_col).copy()\n",
        "\n",
        "    num_comp = embed_df[embed_name][0].shape[0]\n",
        "\n",
        "    if num_comp == 2:\n",
        "        tmp_df['component_0'] = tmp_df.apply(\n",
        "            lambda x: x[embed_name][0], axis=1)\n",
        "        tmp_df['component_1'] = tmp_df.apply(\n",
        "            lambda x: x[embed_name][1], axis=1)\n",
        "        title = embed_name + \\\n",
        "            ' (CP Rank = ' + str(num_comp) + ') Visualisation without T-SNE'\n",
        "        xlabel = embed_name + ' component_0'\n",
        "        ylabel = embed_name + ' component_1'\n",
        "\n",
        "    elif num_comp > 2:\n",
        "        tsne_obj = TSNE(n_components=2, verbose=0, perplexity=perplexity,\n",
        "                        n_iter=1000, random_state=15)\n",
        "\n",
        "        tsne_results = tsne_obj.fit_transform(\n",
        "            np.stack(tmp_df[embed_name].to_numpy(), axis=0))\n",
        "\n",
        "        tsne_user_df = pd.DataFrame(data=tsne_results,\n",
        "                                    index=embed_df[id_col], columns=['component_0', 'component_1'])\n",
        "\n",
        "        tmp_df = tmp_df.join(tsne_user_df, how='left', on=id_col)\n",
        "\n",
        "        title = embed_name + \\\n",
        "            ' (CP Rank = ' + str(num_comp) + ') Visualisation with T-SNE'\n",
        "        xlabel = embed_name + ' T-SNE component_0'\n",
        "        ylabel = embed_name + ' T-SNE component_1'\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(24, 6), dpi=90)\n",
        "    fig.suptitle(title, fontsize=20)\n",
        "\n",
        "    for idx, feat in enumerate(['avg_calories', 'avg_speed', 'avg_distance']):\n",
        "        if embed_name == 'userEmbed':\n",
        "            sns.scatterplot(x='component_0',\n",
        "                            y='component_1',\n",
        "                            palette=sns.color_palette(\n",
        "                                'rocket_r', as_cmap=True),\n",
        "                            data=tmp_df,\n",
        "                            hue=feat,\n",
        "                            ax=axes[idx])\n",
        "\n",
        "        elif embed_name == 'routeEmbed':\n",
        "            sns.scatterplot(x='component_0',\n",
        "                            y='component_1',\n",
        "                            palette='colorblind',\n",
        "                            size=feat,\n",
        "                            sizes=(2, 200),\n",
        "                            data=tmp_df,\n",
        "                            hue='sport',\n",
        "                            ax=axes[idx],\n",
        "                            legend='brief')\n",
        "\n",
        "    for ax in axes:\n",
        "        ax.set_xlabel(xlabel, fontsize=15)\n",
        "        ax.set_ylabel(ylabel, fontsize=15)\n",
        "        ax.legend(loc='lower left',\n",
        "                  title=ax.get_legend().get_title().get_text(),\n",
        "                  fontsize='small')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vPLIsdpu5spm",
        "outputId": "e82a7dbf-75ba-44ad-a4a9-9e14cf0b5d2d"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "\n",
        "num_comps = [2, 11, 13, 16]\n",
        "\n",
        "for num_comp in num_comps:\n",
        "    user_matrix, route_matrix, _, _ = cp_with_eff_corcondia(\n",
        "        users_routes_arr, num_comp)\n",
        "    user_embed_df = pd.DataFrame(zip(user_list, user_matrix), columns=[\n",
        "        'userId', 'userEmbed'])\n",
        "\n",
        "    route_embed_df = data_df[['Route_id']].copy()\n",
        "    route_embed_df.drop_duplicates(inplace=True, ignore_index=True)\n",
        "    route_embed_df['routeEmbed'] = route_embed_df.apply(\n",
        "        lambda x: route_matrix[int(x.Route_id)], axis=1)\n",
        "\n",
        "    embed_visual(user_profile_df, user_embed_df, 'userEmbed', perplexity=30)\n",
        "    embed_visual(route_profile_df, route_embed_df, 'routeEmbed', perplexity=5)\n",
        "\n",
        "elapsed = format_time(time.time() - start)\n",
        "print(elapsed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIrXY2LT9lrI"
      },
      "source": [
        "## 6.3 Save Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wa7-eELT5oDh"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Optimal number of rank is 13 from the analysis above\n",
        "num_comp = 13\n",
        "\n",
        "user_matrix, route_matrix, _, _ = cp_with_eff_corcondia(\n",
        "    users_routes_arr, num_comp)\n",
        "user_embed_df = pd.DataFrame(zip(user_list, user_matrix), columns=[\n",
        "    'userId', 'userEmbed'])\n",
        "\n",
        "route_embed_df = data_df[['Route_id']].copy()\n",
        "route_embed_df.drop_duplicates(inplace=True, ignore_index=True)\n",
        "route_embed_df['routeEmbed'] = route_embed_df.apply(\n",
        "    lambda x: route_matrix[int(x.Route_id)], axis=1)\n",
        "\n",
        "\n",
        "# Save Embeddings\n",
        "\n",
        "userEmbed_file_path = '/content/drive/My Drive/Colab Notebooks/DATA5703/Entity Embedding/userEmbed_tensorD_13.m'\n",
        "routeEmbed_file_path = '/content/drive/My Drive/Colab Notebooks/DATA5703/Entity Embedding/routeEmbed_tensorD_13.m'\n",
        "\n",
        "joblib.dump(user_embed_df, userEmbed_file_path)\n",
        "joblib.dump(route_embed_df, routeEmbed_file_path)\n",
        "\n",
        "# Load Embeddings\n",
        "\n",
        "user_embed_df = joblib.load(userEmbed_file_path)\n",
        "route_embed_df = joblib.load(routeEmbed_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "F90EBv5G5oGy",
        "outputId": "0b561700-18a6-4085-b2df-ab8997f0cc9c"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "user_embed_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "fqF4qy1OFXOx",
        "outputId": "3b99f0d8-bcb7-4e6e-c829-09e06b7616fd"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "route_embed_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mou1oTbF0Efa"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python311\\python.exe' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "cKpUJCfSfoKY",
        "SrxDs0zCf0DT",
        "bjVLdsmif5JB",
        "soJV2ms-KSW5",
        "ggN2gyGtgJJM",
        "Qxo2Ce9hgjHH",
        "R5R6f1lpx4Ls",
        "RMCYFs6ayGb7",
        "gAX-aCyQ5X1_",
        "2rRvl22T5RzS",
        "NIrXY2LT9lrI"
      ],
      "name": "Rev 3 - User Embedding Tensor Decomposition.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
